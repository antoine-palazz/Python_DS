{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f6a79c-0288-4159-b659-967dce3927fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "!pip install --upgrade xlrd #colab bug verson xlrd\n",
    "!pip install geopandas\n",
    "!pip install pandas fiona shapely pyproj rtree\n",
    "!pip install contextily\n",
    "!pip install pygeos\n",
    "!pip install topojson\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "!pip install imbalanced-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "!pip install factor-analyzer\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import plotly.express as px\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845f5954-e5cc-4d83-899f-5f7964bb518e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_dummies.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5dbd8c-f79e-4646-9505-aeda5534caf3",
   "metadata": {},
   "source": [
    "# Regression logistique de la maladie sur l'âge.\n",
    "Il semblait assez évident que l'âge jouerait un rôle déterminant pour la contraction de maladies mais il conevnait de voir \"à quel point\". \n",
    "C'est pourquoi nous avons mené une régression logistique sur la question. \n",
    "Après plusieurs tentatives, c'est avec une méthode de ré échantillonnage et de changements de poids que nous sommes parvenus au résultat le plus précis. \n",
    "En effet, les malades atteints de cancer étant en minorité dans la population, sans modification de leur poids ou échantillonnage différent, il aurait été naturel de classer tous les individus comme bien portants pour minimiser le risque d'erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6e6e4-fbd2-4480-8f45-8a8c27f9a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_regression = df[['indicatrice__Cancers', 'classe_age']].dropna()\n",
    "\n",
    "\n",
    "X = df_regression[['classe_age']]\n",
    "y = df_regression['indicatrice__Cancers']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#re echantillonnage \n",
    "rus = RandomUnderSampler(sampling_strategy=0.5, random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "#On change les poids\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_resampled, y_resampled)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "classification_rep = classification_report(y_test, predictions, zero_division=1)\n",
    "\n",
    "coefficients = model.coef_[0]\n",
    "intercept = model.intercept_[0]\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{classification_rep}')\n",
    "\n",
    "print(f'Coefficients: {coefficients}')\n",
    "print(f'Intercept: {intercept}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c7b1bf-3a05-42b4-ae42-9b6571c7705f",
   "metadata": {},
   "source": [
    "On remarque que pour chaque classe d'âge franchie, les chances de contracter un cancer augmentent de 0.015, ce qui est un coefficient relativement important comparé aux résultats des autres régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188ca175-1291-489c-bc17-92266a32f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_values = np.linspace(X.min(), X.max(), 1000).reshape(-1, 1)\n",
    "probabilities = model.predict_proba(X_values)[:, 1]\n",
    "\n",
    "plt.scatter(X, y, label='Données (peu évocateur)')\n",
    "plt.plot(X_values, probabilities, color='red', label=\"Régression logistique de 'cancer' sur l'âge\")\n",
    "plt.xlabel('Âge')\n",
    "plt.ylabel(\"Probabilité d'avoir un cancer\")\n",
    "plt.title('Courbe de régression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd97a8a-59f0-4429-a978-40984d0239c5",
   "metadata": {},
   "source": [
    "# CLUSTERING/ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bdc17-d116-4db1-aaa5-069e043f6dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster = pd.read_csv('base_modelisation.csv')\n",
    "df_cluster = df_cluster.drop(['Unnamed: 0', 'dept', \n",
    "                              'indicatrice_Hospitalisations ponctuelles (avec ou sans pathologies, traitements ou maternité)'], axis = 1)\n",
    "df_cluster = df_cluster.set_index('dep', drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e217e68-e0ac-4c17-806b-d02e6eba4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On fait l acp et on standardise nos variables\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_cluster)\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "loading_matrix = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=df_cluster.columns)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "#Cercle de correlation\n",
    "circle = plt.Circle((0, 0), 1, fill=False, edgecolor='b', linestyle='dashed')\n",
    "ax.add_patch(circle)\n",
    "for i, var in enumerate(loading_matrix.index):\n",
    "    ax.arrow(0, 0, loading_matrix.loc[var, 'PC1'], loading_matrix.loc[var, 'PC2'],\n",
    "             color='r', alpha=0.7, linewidth=0.5, head_width=0.02, head_length=0.02)\n",
    "    ax.text(loading_matrix.loc[var, 'PC1'] * 1.15, loading_matrix.loc[var, 'PC2'] * 1.15, var,\n",
    "            color='b', ha='center', va='center')\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_title('Cercle de corrélations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86451a82-f4f7-4524-86d0-521c0be6b051",
   "metadata": {},
   "source": [
    "On remarque donc que la classe d'âge a une place unique dans la classification des départements.\n",
    "Par ailleurs, on remarque que beaucoup de variables sont très liées entre elles, c'est pourquoi on effectue une ACP avec la méthode de rotation 'Promax' pour que ce soit plus interprétable et essayer de distinguer s'il n'y a pas d'autres facteurs qui peuvent être également importants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe3c38-5a79-4e47-b4b5-3e5d36c65bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memes operations que precedemment mais avec la methode de rotation promax\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(df_cluster)\n",
    "pca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2'])\n",
    "fa = FactorAnalyzer(rotation='promax', n_factors=2)\n",
    "fa.fit(df_cluster)\n",
    "rotated_loadings = fa.loadings_\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Cercle de correlations\n",
    "circle = plt.Circle((0, 0), 1, fill=False, edgecolor='b', linestyle='dashed')\n",
    "ax.add_patch(circle)\n",
    "for i, var in enumerate(df_cluster.columns):\n",
    "    ax.arrow(0, 0, rotated_loadings[i, 0], rotated_loadings[i, 1],\n",
    "             color='r', alpha=0.7, linewidth=0.5, head_width=0.02, head_length=0.02)\n",
    "    ax.text(rotated_loadings[i, 0] * 1.15, rotated_loadings[i, 1] * 1.15, var,\n",
    "            color='b', ha='center', va='center')\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_title('Circle of Correlation with Promax Rotation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bcaac5-8e6a-43e0-b647-e17b08819d37",
   "metadata": {},
   "source": [
    "On remarque que les variables sont un (tout) petit peu moins groupées. Mais c'est pouruqoi, afin de trouver les variables les plus intéressantes, on garde celles qui contribuent le plus aux différentes composantes principales et on laisse de côté celles qui sont déjà très corrélées aux variables et qui, par ailleurs, contribuent moins aux composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e74907-e340-42f9-9d23-2cead66481ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_to_drop = ['Energie', 'Autres transports international', 'Routier',\n",
    "                'indicatrice_Traitements psychotropes (hors pathologies)',\n",
    "                'indicatrice_Maladies cardio-neurovasculaires',\n",
    "                'indicatrice_Maladies psychiatriques', 'indicatrice_Diabète','indicatrice_Séjours en hospitalisation complète pour prise en charge de la Covid-19',\n",
    "                'indicatrice_Traitements du risque vasculaire (hors pathologies)']\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a42ed-5d13-4404-85b1-fcdff30be293",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_reduit = df_cluster.drop(list_to_drop, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8ddba3-4ee5-4e77-87fc-a0d288cbb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on refait exactement les memes commandes que precemment mais avec un jeu de donnees reduit pour essayer\n",
    "#d ameliorer notre modele\n",
    "pca = PCA(n_components=2)\n",
    "pca_result_bis = pca.fit_transform(df_cluster_reduit)\n",
    "pca_df_reduit = pd.DataFrame(data=pca_result_bis, columns=['PC1', 'PC2'])\n",
    "fa = FactorAnalyzer(rotation='promax', n_factors=2)\n",
    "fa.fit(df_cluster_reduit)\n",
    "rotated_loadings = fa.loadings_\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "circle = plt.Circle((0, 0), 1, fill=False, edgecolor='b', linestyle='dashed')\n",
    "ax.add_patch(circle)\n",
    "for i, var in enumerate(df_cluster_reduit.columns):\n",
    "    ax.arrow(0, 0, rotated_loadings[i, 0], rotated_loadings[i, 1],\n",
    "             color='r', alpha=0.7, linewidth=0.5, head_width=0.02, head_length=0.02)\n",
    "    ax.text(rotated_loadings[i, 0] * 1.15, rotated_loadings[i, 1] * 1.15, var,\n",
    "            color='b', ha='center', va='center')\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_title('Cercle de corrélation avec moins de variables, et rotation Promax')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bed01b5-2f61-48b3-9854-aa1e21b0392f",
   "metadata": {},
   "source": [
    "On remarque que mis à part le titre et une partie des variables assez liées en pollution, le cercle de corrélations est plus lisible. \n",
    "C'est pourquoi on va mener un clustering sur ces composantes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90956069-49b0-4a61-af38-20bc6798cf53",
   "metadata": {},
   "source": [
    "### Clustering k-moyennes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e13324-8a97-496c-af9d-fd438d0efbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(pca_df_reduit)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "#On regarde la methode du coude\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Heuristique du coude')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('Somme des carrés des distances intra cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7799642-99b5-4cfd-9f82-15350db12cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "df_cluster_reduit['Cluster'] = kmeans.fit_predict(pca_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c15ba5-5ec9-4824-a49a-17920cf842fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_df_reduit['PC1'], pca_df_reduit['PC2'], c=df_cluster_reduit['Cluster'], cmap='viridis')\n",
    "plt.title('Clusters faits avec les composantes principales')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e39685-b46f-4a71-9fd4-f9909c4c3a62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cluster_reduit[df_cluster_reduit['Cluster'] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04822fbd-faa4-4dc4-829e-1e8e7e0bbde3",
   "metadata": {},
   "source": [
    "On remarque que mis à part deux cas : Lozère et Territoire de Belfort, c'est surtout la classe d'âge et les maladies (qui sont très liées à l'âge) qui permettent de classer les départements.\n",
    "Pour permettre de mieux voir ces résultats et de constater l'appartenance de chaque département à un certain cluster, nous faisons ce graphique 'interactif'.\n",
    "Le clustering se base toujours sur l'ACP réduite avec la rotation promax. \n",
    "On continue de garder 5 clusters car c'est le nombre de clusters que nous avions retenus pour les cartes, et par ailleurs, la méthode du coude semble indiquer que c'est un bon résultat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0e97f-9246-45a7-9c19-47a3c19c6289",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_reduit_reset = df_cluster_reduit.reset_index()\n",
    "\n",
    "#ACP\n",
    "pca = PCA(n_components=2)\n",
    "pca_result_bis = pca.fit_transform(df_cluster_reduit_reset.drop(columns=['dep']))  \n",
    "pca_df_reduit = pd.DataFrame(data=pca_result_bis, columns=['PC1', 'PC2'])\n",
    "fa = FactorAnalyzer(rotation='promax', n_factors=2)\n",
    "fa.fit(df_cluster_reduit_reset.drop(columns=['dep'])) \n",
    "rotated_loadings = fa.loadings_\n",
    "\n",
    "#Clustering\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(pca_df_reduit)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title('Heuristique du coude')\n",
    "plt.xlabel('Nombre de clusters')\n",
    "plt.ylabel('Somme des carrés des distances intra-clusters')\n",
    "plt.show()\n",
    "kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "df_cluster_reduit_reset['Cluster'] = kmeans.fit_predict(pca_df_reduit)\n",
    "df_combined = pd.concat([df_cluster_reduit_reset[['dep', 'Cluster']], pca_df_reduit], axis=1)\n",
    "\n",
    "# On utilise plotly pour que ce soit interactif\n",
    "fig = px.scatter(df_combined, x='PC1', y='PC2', color='Cluster',\n",
    "                 title='Clusters intéractifs', labels={'Cluster': 'Cluster'},\n",
    "                 color_continuous_scale='viridis', hover_name='dep')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8e4aa-fa0b-49c9-a9ea-3cc6834735b2",
   "metadata": {},
   "source": [
    "# ACP et Clustering sur la tranche des 30-60 ans \n",
    "Comme nous avons remarqué que l'âge avait une place prépondérante dans la détermination de la maladie, nous avons cherché sinon à éliminer l'effet de l'âge, du moins essayer de le minorer pour voir si la pollution avait en effet un impact sur la santé. \n",
    "La classe d'âge choisie est certes hétéroclite mais nous pensons que cela pourrait être plus évocateur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b746904-0641-4a08-bf55-b621baf896c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_30_60 = pd.read_csv('base_modelisation_30_60.csv')\n",
    "df_cluster_30_60 = df_cluster_30_60.drop(['Unnamed: 0', 'dept', \n",
    "                              'indicatrice_Hospitalisations ponctuelles (avec ou sans pathologies, traitements ou maternité)'], axis = 1)\n",
    "df_cluster_30_60 = df_cluster_30_60.set_index('dep', drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8bea88-032a-4397-ac6b-614b7abb946a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_cluster_30_60.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d029fa2b-07bd-46b0-88d3-13406af7c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#il y avait une erreur dans l acp reduite a une partie de la population\n",
    "#on remarque qu'il y a des variables dont la variance est nulle\n",
    "#on peut donc les supprimer comme elle n apportent pas d information pour categoriser la population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea93a63-3000-4589-8dd3-bad2e783ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cluster_30_60 = df_cluster_30_60.drop(['indicatrice_Autres affections de longue durée (dont 31 et 32)',\n",
    "                               'indicatrice_Maladies respiratoires chroniques (hors mucoviscidose)'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190d51e7-b246-424e-a8b1-2dfb8fe7897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_30_60 = PCA(n_components=2)\n",
    "pca_result_30_60 = pca_30_60.fit_transform(df_cluster_30_60)\n",
    "pca_df_30_60 = pd.DataFrame(data=pca_result_30_60, columns=['Custom_PC1', 'Custom_PC2'])  # Change variable names\n",
    "\n",
    "# Promax rotation\n",
    "fa = FactorAnalyzer(rotation='promax', n_factors=2)\n",
    "fa.fit(df_cluster_30_60)\n",
    "rotated_loadings = fa.loadings_\n",
    "\n",
    "# Plot the Circle of Correlation with Promax rotation\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot circle\n",
    "circle = plt.Circle((0, 0), 1, fill=False, edgecolor='b', linestyle='dashed')\n",
    "ax.add_patch(circle)\n",
    "\n",
    "# Plot arrows for each variable after rotation\n",
    "for i, var in enumerate(df_cluster_30_60.columns):\n",
    "    ax.arrow(0, 0, rotated_loadings[i, 0], rotated_loadings[i, 1],\n",
    "             color='r', alpha=0.7, linewidth=0.5, head_width=0.02, head_length=0.02)\n",
    "    ax.text(rotated_loadings[i, 0] * 1.15, rotated_loadings[i, 1] * 1.15, var,\n",
    "            color='b', ha='center', va='center')\n",
    "\n",
    "ax.set_xlim([-1, 1])\n",
    "ax.set_ylim([-1, 1])\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.set_title('Circle of Correlation with Promax Rotation for df_cluster_30_60')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c7aa18-c871-4d1e-9bc5-fe77d6b053a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#on remarque que l age a encore une place preponderante dans la \n",
    "#constitution des composantes principales "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92ef4b-41ef-4638-828a-4db2195131bb",
   "metadata": {},
   "source": [
    "# Regression logistique bis, cas multivarié avec plusieurs facteurs explicatifs, et en prenant en compte la pollution\n",
    "On procède cette fois à l'échelle individuelle pour essayer de mettre plus en avant l'effet de la pollution non pas à l'échelle agrégée mais de chaque patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e2c9b9-16ae-4d85-8baa-8c61dbd5b05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = pd.read_csv(\"data_reg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca81b97-fab8-420d-b16e-12e9ea1b34fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg = data_reg.drop(['Unnamed: 0', 'ind', 'dept'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfcf46c-145b-423c-a383-0d4f7a16601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6a7799-fe75-4341-b129-9b995bb4b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_cancer = data_reg.drop(['indicatrice_Maladies cardio-neurovasculaires',\n",
    "                                'indicatrice_Maladies respiratoires chroniques (hors mucoviscidose)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691f9a1-afb1-4f5b-abeb-7a08ba184de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_neuro = data_reg.drop(['indicatrice_Cancers',\n",
    "                                'indicatrice_Maladies respiratoires chroniques (hors mucoviscidose)'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ec338-1113-448d-8285-29c3d0c2a7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_resp =  data_reg.drop(['indicatrice_Cancers',\n",
    "                                'indicatrice_Maladies cardio-neurovasculaires'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb561e4-6d60-4fe0-bbd2-147ab18186c9",
   "metadata": {},
   "source": [
    "#### Régression cancer bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70171130-12f1-44b1-b277-8b65416488dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_reg_cancer.drop('indicatrice_Cancers', axis=1)\n",
    "y = data_reg_cancer['indicatrice_Cancers']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "logreg_model = LogisticRegression(class_weight={0: 1, 1: 5.5}) #Parce qu on s intersse aux cas ou la variable \n",
    "#est egale a un et si on ne balance pas comme ca, comme les 0 sont majoritaires, la regression classera tout le monde en 0\n",
    "\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883addc-dbed-4d50-a95e-4a5a6fb5db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'data_reg_cancer' with variables 'indicatrice_Cancers', 'feature1', 'feature2', etc.\n",
    "# Replace these column names with your actual column names.\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data_reg_cancer.drop('indicatrice_Cancers', axis=1)\n",
    "y = data_reg_cancer['indicatrice_Cancers']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Manually set class weights\n",
    "logreg_model = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "\n",
    "# Fit the model on the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Display the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train the model using the resampled data\n",
    "logreg_model_resampled = LogisticRegression()\n",
    "logreg_model_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the test data using the resampled model\n",
    "y_pred_resampled = logreg_model_resampled.predict(X_test)\n",
    "\n",
    "# Display the confusion matrix and classification report for the resampled model\n",
    "print(\"\\nConfusion Matrix (Resampled Model):\")\n",
    "print(confusion_matrix(y_test, y_pred_resampled))\n",
    "\n",
    "print(\"\\nClassification Report (Resampled Model):\")\n",
    "print(classification_report(y_test, y_pred_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd67f5e-213d-4a40-b5bf-0247c6a8a525",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg_model_resampled.coef_[0]\n",
    "intercept = logreg_model_resampled.intercept_[0]\n",
    "\n",
    "# Create a DataFrame to display the coefficients and their effects\n",
    "coefficients_df = pd.DataFrame({'Variable': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# Calculate the odds ratio and its effect\n",
    "coefficients_df['Odds Ratio'] = coefficients_df['Coefficient'].apply(lambda x: round(np.exp(x), 4))\n",
    "coefficients_df['Effect'] = coefficients_df['Odds Ratio'].apply(lambda x: f\"Increases by {x} times\" if x > 1 else f\"Decreases by {round(1/x, 4)} times\")\n",
    "\n",
    "# Display the coefficients and their effects\n",
    "print(coefficients_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4737e478-1589-4a49-9d80-505a1040e041",
   "metadata": {},
   "source": [
    "### Regression maladies respiratoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f3304-1fad-417c-9e98-acf312220728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'data_reg_cancer' with variables 'indicatrice_Cancers', 'feature1', 'feature2', etc.\n",
    "# Replace these column names with your actual column names.\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data_reg_resp.drop('indicatrice_Maladies respiratoires chroniques (hors mucoviscidose)', axis=1)\n",
    "y = data_reg_resp['indicatrice_Maladies respiratoires chroniques (hors mucoviscidose)']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Manually set class weights\n",
    "logreg_model = LogisticRegression(class_weight={0: 1, 1: 5})\n",
    "\n",
    "# Fit the model on the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Display the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train the model using the resampled data\n",
    "logreg_model_resampled = LogisticRegression()\n",
    "logreg_model_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the test data using the resampled model\n",
    "y_pred_resampled = logreg_model_resampled.predict(X_test)\n",
    "\n",
    "# Display the confusion matrix and classification report for the resampled model\n",
    "print(\"\\nConfusion Matrix (Resampled Model):\")\n",
    "print(confusion_matrix(y_test, y_pred_resampled))\n",
    "\n",
    "print(\"\\nClassification Report (Resampled Model):\")\n",
    "print(classification_report(y_test, y_pred_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e5e29-92a8-4181-bb25-25daeca1e87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg_model_resampled.coef_[0]\n",
    "intercept = logreg_model_resampled.intercept_[0]\n",
    "\n",
    "# Create a DataFrame to display the coefficients and their effects\n",
    "coefficients_df = pd.DataFrame({'Variable': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# Calculate the odds ratio and its effect\n",
    "coefficients_df['Odds Ratio'] = coefficients_df['Coefficient'].apply(lambda x: round(np.exp(x), 4))\n",
    "coefficients_df['Effect'] = coefficients_df['Odds Ratio'].apply(lambda x: f\"Increases by {x} times\" if x > 1 else f\"Decreases by {round(1/x, 4)} times\")\n",
    "\n",
    "# Display the coefficients and their effects\n",
    "print(coefficients_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939a5ef3-3be8-4277-86b4-cbcbdd86f824",
   "metadata": {},
   "source": [
    "### Régression maladies neurologiques et cardiaques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d25449-f4b2-4aac-ad14-5c2c6b6a8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'data_reg_cancer' with variables 'indicatrice_Cancers', 'feature1', 'feature2', etc.\n",
    "# Replace these column names with your actual column names.\n",
    "\n",
    "# Separate the features and target variable\n",
    "X = data_reg_neuro.drop('indicatrice_Maladies cardio-neurovasculaires', axis=1)\n",
    "y = data_reg_neuro['indicatrice_Maladies cardio-neurovasculaires']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Manually set class weights\n",
    "logreg_model = LogisticRegression(class_weight={0: 1, 1: 10})\n",
    "\n",
    "# Fit the model on the training data\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = logreg_model.predict(X_test)\n",
    "\n",
    "# Display the confusion matrix and classification report\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Apply SMOTE (Synthetic Minority Over-sampling Technique)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create and train the model using the resampled data\n",
    "logreg_model_resampled = LogisticRegression()\n",
    "logreg_model_resampled.fit(X_resampled, y_resampled)\n",
    "\n",
    "# Make predictions on the test data using the resampled model\n",
    "y_pred_resampled = logreg_model_resampled.predict(X_test)\n",
    "\n",
    "# Display the confusion matrix and classification report for the resampled model\n",
    "print(\"\\nConfusion Matrix (Resampled Model):\")\n",
    "print(confusion_matrix(y_test, y_pred_resampled))\n",
    "\n",
    "print(\"\\nClassification Report (Resampled Model):\")\n",
    "print(classification_report(y_test, y_pred_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a0947-2690-430c-a07a-8007e9ed1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = logreg_model_resampled.coef_[0]\n",
    "intercept = logreg_model_resampled.intercept_[0]\n",
    "\n",
    "# Create a DataFrame to display the coefficients and their effects\n",
    "coefficients_df = pd.DataFrame({'Variable': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# Calculate the odds ratio and its effect\n",
    "coefficients_df['Odds Ratio'] = coefficients_df['Coefficient'].apply(lambda x: round(np.exp(x), 4))\n",
    "coefficients_df['Effect'] = coefficients_df['Odds Ratio'].apply(lambda x: f\"Increases by {x} times\" if x > 1 else f\"Decreases by {round(1/x, 4)} times\")\n",
    "\n",
    "# Display the coefficients and their effects\n",
    "print(coefficients_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f442ad7e-dc0c-45c3-a2e9-9b37042d38cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
